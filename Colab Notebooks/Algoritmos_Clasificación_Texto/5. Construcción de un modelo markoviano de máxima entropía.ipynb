{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. Construcción de un modelo markoviano de máxima entropía.ipynb","provenance":[{"file_id":"1OJDUVr4LFU4FAUwOW2oJnIHo5-I-x9Nc","timestamp":1639170587968}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JCL0r4ap2H4T"},"source":["## Construcción de un modelo markoviano de máxima entropía\n"]},{"cell_type":"code","metadata":{"id":"7FaYXYmy5ahC"},"source":["!pip install conllu\n","!pip install stanza\n","!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Oi9R1bn2Qmg"},"source":["### Entrenamiento del modelo - cálculo de conteos\n","\n","Parta este modelo consideramos el cálculo de las probabilidades: \n","\n","$$P(t_i | w_i, t_{i-1}) =\\frac{C(w_i, t_i, t_{i-1})}{C(w_i, t_{i-1})} $$\n","\n","* `uniqueFeatureDict` $C(tag|word,prevtag) = C(w_i, t_i, t_{i-1})$\n","* `contextDict` $C(word,prevtag) = C(w_i, t_{i-1})$\n","\n","En este caso cuando consideremos el primer elemento de una frase $w_0$, no existirá un elemento anterior $w_{-1}$ y por lo tanto, tampoco una etiqueta previa $t_{-1}$, podemos modelar este problema asumiendo que existe una etiqueta \"None\", para estos casos: \n","\n","$$P(t_0|w_0,t_{-1}) = P(t_0|w_0,\\text{\"None\"})$$"]},{"cell_type":"code","metadata":{"id":"0riVspXk1nHl"},"source":["from conllu import parse_incr\n","\n","uniqueFeatureDict = {}\n","contextDict = {}\n","\n","tagtype = 'upos'\n","data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n","\n","# Calculando conteos (pre-probabilidades)\n","for tokenlist in parse_incr(data_file):\n","  prevtag = \"None\"\n","  for token in tokenlist:\n","    tag = token[tagtype]\n","    word = token['form'].lower()\n","    #C(tag|word,prevtag)\n","    largeKey = tag+'|'+word+','+prevtag\n","    if largeKey in uniqueFeatureDict.keys():\n","      uniqueFeatureDict[largeKey]+=1\n","    else:\n","      uniqueFeatureDict[largeKey]=1\n","    key = word+','+prevtag\n","    if key in contextDict.keys():\n","      contextDict[key]+=1\n","    else:\n","      contextDict[key]=1\n","    #print(largeKey, key, '\\n')\n","    prevtag=tag"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6npPBeu_8OzK"},"source":["### Entrenamiento del modelo - cálculo de probabilidades\n","\n","$$P(t_i|w_i, t_{i-1}) = \\frac{C(t_i, w_i, t_{i-1})}{C(w_i, t_{i-1})}$$"]},{"cell_type":"code","metadata":{"id":"ywGEbMTw73NU"},"source":["posteriorProbDict = {}\n","\n","for key in uniqueFeatureDict.keys():\n","  if len(key.split('|'))==3:\n","    posteriorProbDict[key] = uniqueFeatureDict[key]/contextDict['|'+key.split('|')[-1]]\n","  else:\n","    posteriorProbDict[key] = uniqueFeatureDict[key]/contextDict[key.split('|')[1]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eA9wqVQPBiVN"},"source":["# Aquí verificamos que todas las probabilidades \n","# por cada contexto 'word,prevtag' suman 1.0\n"," \n","for base_context in contextDict.keys():\n","  countprob = 0\n","  items = 0\n","  for key in posteriorProbDict.keys():\n","    if len(key.split('|'))==3:\n","      if '|'+key.split('|')[-1]==base_context:\n","        countprob+=posteriorProbDict[key]\n","        items+=1\n","    else:\n","      if key.split('|')[1]==base_context:\n","        countprob+=posteriorProbDict[key]\n","        items+=1\n","  print(base_context, items, countprob)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leSuajlOA-3c"},"source":["### Distribución inicial de estados latentes"]},{"cell_type":"code","metadata":{"id":"nU4ShPmDQRp0","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597717505862,"user_tz":300,"elapsed":1202,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"36c33c52-c49a-482b-e7b4-240e25bbf92f"},"source":["# identificamos las categorias gramaticales 'upos' unicas en el corpus\n","stateSet = {'ADJ',\n"," 'ADP',\n"," 'ADV',\n"," 'AUX',\n"," 'CCONJ',\n"," 'DET',\n"," 'INTJ',\n"," 'NOUN',\n"," 'NUM',\n"," 'PART',\n"," 'PRON',\n"," 'PROPN',\n"," 'PUNCT',\n"," 'SCONJ',\n"," 'SYM',\n"," 'VERB',\n"," '_'}\n","# enumeramos las categorias con numeros para asignar a \n","# las columnas de la matriz de Viterbi\n","tagStateDict = {}\n","for i, state in enumerate(stateSet):\n","  tagStateDict[state] = i\n","tagStateDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ADJ': 9,\n"," 'ADP': 8,\n"," 'ADV': 1,\n"," 'AUX': 14,\n"," 'CCONJ': 4,\n"," 'DET': 12,\n"," 'INTJ': 15,\n"," 'NOUN': 2,\n"," 'NUM': 7,\n"," 'PART': 3,\n"," 'PRON': 5,\n"," 'PROPN': 0,\n"," 'PUNCT': 10,\n"," 'SCONJ': 6,\n"," 'SYM': 16,\n"," 'VERB': 11,\n"," '_': 13}"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"oACerPVe_Awz","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597717527063,"user_tz":300,"elapsed":9828,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"ee5372eb-1f00-4b05-81b1-68fabcaa28d8"},"source":["initTagStateProb = {} # \\rho_i^{(0)}\n","from conllu import parse_incr \n","wordList = []\n","data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n","count = 0 # cuenta la longitud del corpus\n","for tokenlist in parse_incr(data_file):\n","  count += 1\n","  tag = tokenlist[0]['upos']\n","  if tag in initTagStateProb.keys():\n","    initTagStateProb[tag] += 1\n","  else:\n","    initTagStateProb[tag] = 1\n","\n","for key in initTagStateProb.keys():\n","  initTagStateProb[key] /= count\n","\n","initTagStateProb"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ADJ': 0.010136315973435861,\n"," 'ADP': 0.1574274729115694,\n"," 'ADV': 0.07577770010485844,\n"," 'AUX': 0.022789234533379936,\n"," 'CCONJ': 0.036980076896190144,\n"," 'DET': 0.34799021321216356,\n"," 'INTJ': 0.0020272631946871723,\n"," 'NOUN': 0.025026214610276126,\n"," 'NUM': 0.0068507514854945824,\n"," 'PART': 0.002446696959105208,\n"," 'PRON': 0.04173365955959455,\n"," 'PROPN': 0.10506815798671792,\n"," 'PUNCT': 0.09143656064313177,\n"," 'SCONJ': 0.027123383432366307,\n"," 'SYM': 0.0004893393918210416,\n"," 'VERB': 0.04557846906675987,\n"," '_': 0.0011184900384480952}"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"neEbST9IDq8f"},"source":["### Construcción del algoritmo de Viterbi\n","\n","Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n","\n","$$\n","\\begin{array}{c c}\n","\\begin{array}{c c c c}\n","\\text{ADJ} \\\\\n","\\text{ADV}\\\\\n","\\text{PRON} \\\\\n","\\vdots \\\\\n","{}\n","\\end{array} \n","&\n","\\left[\n","\\begin{array}{c c c c}\n","\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n","\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n","\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n","\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n","p_1 & p_2 & \\dots & p_n \n","\\end{array}\n","\\right] \n","\\end{array}\n","$$\n","\n","Donde las probabilidades de Viterbi en la primera columna (para una categoria $i$) están dadas por: \n","\n","$$\n","\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times P(i \\vert p_1, \\text{\"None\"})\n","$$\n","\n","y para las siguientes columnas: \n","\n","$$\n","\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times P(j \\vert p_t, i) \\}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"o7fDedW5BE-q","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1597716872708,"user_tz":300,"elapsed":9678,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"524b045a-f059-49b3-ee39-c416c4bf03a6"},"source":["import numpy as np \n","import stanza\n","stanza.download('es')\n","nlp = stanza.Pipeline('es', processors='tokenize')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.28MB/s]                    \n","2020-08-18 02:14:23 INFO: Downloading default packages for language: es (Spanish)...\n","2020-08-18 02:14:25 INFO: File exists: /root/stanza_resources/es/default.zip.\n","2020-08-18 02:14:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"X8yBXTq-PHGY","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597717820959,"user_tz":300,"elapsed":1128,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"a20e1dff-14a4-463c-b2a0-db03c9fbcab6"},"source":["def ViterbiMatrix(secuencia, posteriorProbDict=posteriorProbDict, initTagStateProb=initTagStateProb):\n","  doc = nlp(secuencia)\n","  if len(doc.sentences)>1:\n","    raise ValueError('secuencia must be a string!')\n","  seq = [word.text for word in doc.sentences[0].words]\n","  viterbiProb = np.zeros((17, len(seq)))\n","  \n","  # inicialización primera columna\n","  for tag in tagStateDict.keys():\n","    tag_row = tagStateDict[tag]\n","    key = tag+'|'+seq[0].lower()+','+\"None\"\n","    try:\n","      viterbiProb[tag_row, 0] = initTagStateProb[tag]*posteriorProbDict[key]\n","    except: \n","      pass\n","  \n","  # computo de las siguientes columnas\n","  for col in range(1, len(seq)):\n","    for tag in tagStateDict.keys():\n","      tag_row = tagStateDict[tag]\n","      possible_probs = []\n","      for prevtag in tagStateDict.keys(): \n","        prevtag_row = tagStateDict[prevtag]\n","        key = tag+'|'+seq[col].lower()+','+prevtag\n","        try:\n","          possible_probs.append(\n","              viterbiProb[prevtag_row, col-1]*posteriorProbDict[key])\n","        except:\n","          possible_probs.append(0)\n","      viterbiProb[tag_row, col] = max(possible_probs)\n","\n","  return viterbiProb\n","\n","ViterbiMatrix('el mundo es pequeño')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.00000000e+00, 8.22024126e-03, 1.13643888e-04, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 3.39769972e-01, 5.94927966e-03, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.33820692e-01],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [3.47990213e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 3.33820692e-01, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"XKC3bh60X3h3","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597717827970,"user_tz":300,"elapsed":1004,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"cb65ae08-fdd4-4103-9572-6f77256bf247"},"source":["def ViterbiTags(secuencia, posteriorProbDict=posteriorProbDict, initTagStateProb=initTagStateProb):\n","  doc = nlp(secuencia)\n","  if len(doc.sentences)>1:\n","    raise ValueError('secuencia must be a string!')\n","  seq = [word.text for word in doc.sentences[0].words]\n","  viterbiProb = np.zeros((17, len(seq)))\n","  \n","  # inicialización primera columna\n","  for tag in tagStateDict.keys():\n","    tag_row = tagStateDict[tag]\n","    key = tag+'|'+seq[0].lower()+','+\"None\"\n","    try:\n","      viterbiProb[tag_row, 0] = initTagStateProb[tag]*posteriorProbDict[key]\n","    except: \n","      pass\n","\n","  \n","  # computo de las siguientes columnas\n","  for col in range(1, len(seq)):\n","    for tag in tagStateDict.keys():\n","      tag_row = tagStateDict[tag]\n","      possible_probs = []\n","      for prevtag in tagStateDict.keys(): \n","        prevtag_row = tagStateDict[prevtag]\n","        key = tag+'|'+seq[col].lower()+','+prevtag\n","        try:\n","          possible_probs.append(\n","              viterbiProb[prevtag_row, col-1]*posteriorProbDict[key])\n","        except:\n","          possible_probs.append(0)\n","      viterbiProb[tag_row, col] = max(possible_probs)\n","\n","  # contruccion de secuencia de tags\n","  res = []\n","  for i, p in enumerate(seq):\n","    for tag in tagStateDict.keys():\n","      if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n","        res.append((p, tag))\n","\n","\n","  return res\n","\n","ViterbiTags('el mundo es pequeño')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('el', 'DET'), ('mundo', 'NOUN'), ('es', 'AUX'), ('pequeño', 'ADJ')]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"4OjxWFgVZYKF","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1597717831434,"user_tz":300,"elapsed":892,"user":{"displayName":"Francisco Camacho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64","userId":"03320326049189164988"}},"outputId":"982899cc-8654-4a69-daff-7c0c1b2eb3d2"},"source":["ViterbiTags('estos instrumentos han de rasgar')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('estos', 'DET'),\n"," ('instrumentos', 'NOUN'),\n"," ('han', 'AUX'),\n"," ('de', 'ADP'),\n"," ('rasgar', 'PROPN')]"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"ozD_EqlbaMRB"},"source":["## ¿ Siguientes Pasos ? \n","\n","El modelo construido, aunque es la base de un MEMM, no explota todo el potencial del concepto  que estos modelos representan, en nuestro caso sencillo consideramos solo un **feature** para predecir la categoría gramatical: $<w_i, t_{i-1}>$. Es decir, las probabilidades de una cierta etiqueta $t_i$ dada una observación $<w_i, t_{i-1}>$ se calculan contando eventos donde se observe que $<w_i, t_{i-1}>$ sucede simultáneamente con $t_i$. \n","\n","La generalización de esto (donde puedo considerar multiples observaciones o **features**, y a partir de estos inferir la categoría gramatical) se hace construyendo las llamadas **feature-functions**, donde estas funciones toman valores de 0 o 1, cuando se cumplan las condiciones de la observación o feature en cuestion. En general podemos considerar una **feature-function** como : \n","\n","$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n","\\begin{cases}\n","  1 , & \\text{se cumple condición } a \\\\\n","  0, & \\text{en caso contrario}\n","\\end{cases}\n","$$\n","\n","donde la condición $a$ es una relacion entre los valores que tome $\\text{tag}$ y $\\text{context}$, por ejemplo:\n","\n","$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n","\\begin{cases}\n","  1 , & (t_i, t_{i-1}) = \\text{('VERB', 'ADJ')} \\\\\n","  0, & \\text{en caso contrario}\n","\\end{cases}\n","$$\n","\n","Al considerar varias funciones, y por lo tanto varios features observables, consideramos una combinacion lineal de estos por medio de un coeficiente que multiplique a cada función: \n","\n","$$\n","\\theta_1 f_1(t, o) + \\theta_2 f_2(t, o) + \\dots\n","$$\n","\n","donde los coeficientes indicarán cuales features son más relevantes y por lo tanto pesan más para la decisión del resultado del modelo. De esta manera los coeficientes $\\theta_j$ se vuelven parámetros del modelo que deben ser optimizados (esto puede realizarse con cualquier técnica de optimizacion como el Gradiente Descendente). Ahora, las probabilidades que pueden obtener usando un softmax sobre estas combinaciones lineales de features: \n","\n","$$\n","P = \\prod_i \\frac{\\exp{\\left(\\sum_j \\theta_j f_j(t_i, o)\\right)}}{\\sum_{t'}\\exp{\\left(\\sum_j \\theta_j f_j(t', o)\\right)}}\n","$$\n","\n","Así, lo que buscamos con el algoritmo de optimización es encontrar los parámetros $\\theta_j$ que maximizan la probabilidad anterior. En NLTK encontramos la implementación completa de un clasificador de máxima entropia que no esta restringido a relaciones markovianas: https://www.nltk.org/_modules/nltk/classify/maxent.html\n","\n","Un segmento resumido de la clase en python que implementa este clasificador en NLTK lo encuentras así: \n","\n","```\n","class MaxentClassifier(ClassifierI):\n","\n","    def __init__(self, encoding, weights, logarithmic=True):\n","        self._encoding = encoding\n","        self._weights = weights\n","        self._logarithmic = logarithmic\n","        assert encoding.length() == len(weights)\n","\n","    def labels(self):\n","        return self._encoding.labels()\n","\n","    def set_weights(self, new_weights):\n","        self._weights = new_weights\n","        assert self._encoding.length() == len(new_weights)\n","\n","\n","    def weights(self):\n","        return self._weights\n","\n","    def classify(self, featureset):\n","        return self.prob_classify(featureset).max()\n","\n","    def prob_classify(self, featureset):\n","        ### ...\n","\n","        # Normalize the dictionary to give a probability distribution\n","        return DictionaryProbDist(prob_dict, log=self._logarithmic, normalize=True)\n","\n","    @classmethod\n","    def train(\n","        cls,\n","        train_toks,\n","        algorithm=None,\n","        trace=3,\n","        encoding=None,\n","        labels=None,\n","        gaussian_prior_sigma=0,\n","        **cutoffs\n","    ):\n","     ### ......\n","```\n","\n","Donde te das cuenta de la forma que tienen las clases en NLTK que implementan clasificadores generales. Aquí vemos que la clase `MaxentClassifier` es una subclase de una más general `ClassifierI` la cual representa el proceso de clasificación general de categoría única (es decir, que a cada data-point le corresponda solo una categoria), también que esta clase depende de definir un `encoding`\n"," y unos pesos `weights` : \n","\n","```\n","class MaxentClassifier(ClassifierI):\n","\n","    def __init__(self, encoding, weights, logarithmic=True):\n","```\n","\n","los pesos corresponden a los parámetros $\\theta_i$. Y el encoding es el que corresponde a las funciones $f_a(t, o)$ que dan como resultado valores binarios $1$ o $0$.\n","\n","La documentación de NLTK te puede dar mas detalles de esta implementación: https://www.nltk.org/api/nltk.classify.html\n","\n","Finalmente, un ejemplo completo de uso y mejora de un modelo de máxima entropía, lo puedes encontrar en este fork que guarde especialmente para el curso, para que lo tengas de referencia y puedas jugar y aprender con él: \n","\n","https://github.com/pachocamacho1990/nltk-maxent-pos-tagger\n","\n","El cual fue desarrollado originalmente por Arne Neumann (https://github.com/arne-cl) basado en los fueatures propuestos por Ratnaparki en 1996 para la tarea de etiquetado por categorias gramaticales.\n"]},{"cell_type":"code","metadata":{"id":"mU5SDxi3F819"},"source":[""],"execution_count":null,"outputs":[]}]}